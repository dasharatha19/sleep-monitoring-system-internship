{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3c93e6",
   "metadata": {},
   "source": [
    "CELL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb75734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# CELL 1: WFLW Dataset\n",
    "class WFLWDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                landmarks = np.array(parts[:196], dtype=np.float32).reshape(-1, 2)  # 98 points\n",
    "                pose = np.array(parts[196:199], dtype=np.float32)  # Pitch, yaw, roll\n",
    "                img_path = parts[-1]\n",
    "                full_img_path = os.path.join(self.img_dir, img_path)\n",
    "                if os.path.exists(full_img_path):\n",
    "                    self.data.append((img_path, landmarks, pose))\n",
    "                else:\n",
    "                    print(f\"Warning: Image not found: {full_img_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, landmarks, pose = self.data[idx]\n",
    "        img = cv2.imread(os.path.join(self.img_dir, img_path))\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        img = img.transpose(2, 0, 1) / 255.0\n",
    "        landmarks = landmarks / img.shape[1]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (torch.tensor(img, dtype=torch.float32), \n",
    "                torch.tensor(landmarks.flatten(), dtype=torch.float32), \n",
    "                torch.tensor(pose, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d0cab",
   "metadata": {},
   "source": [
    "CELL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Visualize Sample WFLW Image\n",
    "def visualize_sample_wflw():\n",
    "    annotation_file = 'F:/human behaviour detection/widerface/WFLW/WFLW_annotations/list_98pt_rect_attr_train_test/list_98pt_rect_attr_train.txt'\n",
    "    img_dir = 'F:/human behaviour detection/widerface/WFLW/WFLW_images'\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        line = f.readline().strip()\n",
    "        parts = line.split()\n",
    "        img_path = parts[-1]\n",
    "        landmarks = np.array(parts[:196], dtype=np.float32).reshape(-1, 2)\n",
    "        pose = np.array(parts[196:199], dtype=np.float32)\n",
    "        full_img_path = os.path.join(img_dir, img_path)\n",
    "        if os.path.exists(full_img_path):\n",
    "            img = cv2.imread(full_img_path)\n",
    "            if img is not None:\n",
    "                h, w = img.shape[:2]\n",
    "                landmarks = landmarks * [w, h]\n",
    "                for (x, y) in landmarks:\n",
    "                    cv2.circle(img, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "                cv2.putText(img, f'Pitch: {pose[0]:.2f}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                cv2.putText(img, f'Yaw: {pose[1]:.2f}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                cv2.putText(img, f'Roll: {pose[2]:.2f}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                cv2.imshow('Sample WFLW Image', img)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "                print(f\"Visualized: {full_img_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to load: {full_img_path}\")\n",
    "        else:\n",
    "            print(f\"Image not found: {full_img_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6e854",
   "metadata": {},
   "source": [
    "cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Lightweight HRNet Model\n",
    "class LightweightHRNet(nn.Module):\n",
    "    def __init__(self, num_landmarks=98):\n",
    "        super(LightweightHRNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.stage2_high = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.stage2_low = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.upsample = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_conv = nn.Conv2d(16, 16, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc_landmarks = nn.Linear(16, num_landmarks * 2)\n",
    "        self.fc_pose = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.stage1(x)\n",
    "        x_high = self.stage2_high(x)\n",
    "        x_low = self.downsample(x)\n",
    "        x_low = self.stage2_low(x_low)\n",
    "        x_low_up = self.upsample(x_low)\n",
    "        x = x_high + x_low_up\n",
    "        x = self.final_conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        landmarks = self.fc_landmarks(x)\n",
    "        pose = self.fc_pose(x)\n",
    "        return landmarks, pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafeac9",
   "metadata": {},
   "source": [
    "cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Train HRNet on WFLW\n",
    "def train_pose():\n",
    "    train_dataset = WFLWDataset(\n",
    "        annotation_file='F:/human behaviour detection/widerface/WFLW/WFLW_annotations/list_98pt_rect_attr_train_test/list_98pt_rect_attr_train.txt',\n",
    "        img_dir='F:/human behaviour detection/widerface/WFLW/WFLW_images'\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_dataset = WFLWDataset(\n",
    "        annotation_file='F:/human behaviour detection/widerface/WFLW/WFLW_annotations/list_98pt_rect_attr_train_test/list_98pt_rect_attr_test.txt',\n",
    "        img_dir='F:/human behaviour detection/widerface/WFLW/WFLW_images'\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LightweightHRNet().to(device)\n",
    "    criterion_lm = nn.MSELoss()\n",
    "    criterion_pose = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        train_loss_lm, train_loss_pose = 0.0, 0.0\n",
    "        for images, landmarks, poses in train_loader:\n",
    "            images, landmarks, poses = images.to(device), landmarks.to(device), poses.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_lm, pred_pose = model(images)\n",
    "            loss_lm = criterion_lm(pred_lm, landmarks)\n",
    "            loss_pose = criterion_pose(pred_pose, poses)\n",
    "            loss = loss_lm + 0.1 * loss_pose\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_lm += loss_lm.item()\n",
    "            train_loss_pose += loss_pose.item()\n",
    "        model.eval()\n",
    "        val_loss_lm, val_loss_pose = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, landmarks, poses in val_loader:\n",
    "                images, landmarks, poses = images.to(device), landmarks.to(device), poses.to(device)\n",
    "                pred_lm, pred_pose = model(images)\n",
    "                val_loss_lm += criterion_lm(pred_lm, landmarks).item()\n",
    "                val_loss_pose += criterion_pose(pred_pose, poses).item()\n",
    "        print(f\"Epoch [{epoch+1}/30], Train LM Loss: {train_loss_lm/len(train_loader):.4f}, \"\n",
    "              f\"Train Pose Loss: {train_loss_pose/len(train_loader):.4f}, \"\n",
    "              f\"Val LM Loss: {val_loss_lm/len(val_loader):.4f}, \"\n",
    "              f\"Val Pose Loss: {val_loss_pose/len(val_loader):.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"landmark_pose_model.pt\")\n",
    "    print(\"Model saved as landmark_pose_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75952a4",
   "metadata": {},
   "source": [
    "cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c102dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Main Execution\n",
    "if __name__ == '__main__':\n",
    "    visualize_sample_wflw()\n",
    "    train_pose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3f631",
   "metadata": {},
   "source": [
    "cell 6(testing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daa69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# CELL 1: WFLW Dataset\n",
    "class WFLWDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.category_counts = defaultdict(int)\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                landmarks = np.array(parts[:196], dtype=np.float32).reshape(-1, 2)  # 98 points\n",
    "                pose = np.array(parts[196:199], dtype=np.float32)  # Pitch, yaw, roll\n",
    "                img_path = parts[-1]\n",
    "                full_img_path = os.path.join(self.img_dir, img_path)\n",
    "                if os.path.exists(full_img_path):\n",
    "                    self.data.append((img_path, landmarks, pose))\n",
    "                    category = img_path.split('/')[0]\n",
    "                    self.category_counts[category] += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Image not found: {full_img_path}\")\n",
    "        print(f\"Loaded {len(self.data)} images for testing.\")\n",
    "        print(\"Categories in test set:\", {k: v for k, v in sorted(self.category_counts.items())})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, landmarks, pose = self.data[idx]\n",
    "        img = cv2.imread(os.path.join(self.img_dir, img_path))\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        img = img.transpose(2, 0, 1) / 255.0\n",
    "        landmarks = landmarks / img.shape[1]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(landmarks.flatten(), dtype=torch.float32), torch.tensor(pose), img_path\n",
    "\n",
    "# CELL 2: Lightweight HRNet Model\n",
    "class LightweightHRNet(nn.Module):\n",
    "    def __init__(self, num_landmarks=98):\n",
    "        super(LightweightHRNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.stage2_high = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.stage2_low = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.upsample = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_conv = nn.Conv2d(16, 16, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc_landmarks = nn.Linear(16, num_landmarks * 2)\n",
    "        self.fc_pose = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.stage1(x)\n",
    "        x_high = self.stage2_high(x)\n",
    "        x_low = self.downsample(x)\n",
    "        x_low = self.stage2_low(x_low)\n",
    "        x_low_up = self.upsample(x_low)\n",
    "        x = x_high + x_low_up\n",
    "        x = self.final_conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        landmarks = self.fc_landmarks(x)\n",
    "        pose = self.fc_pose(x)\n",
    "        return landmarks, pose\n",
    "\n",
    "# CELL 3: Compute NME for Landmarks\n",
    "def compute_nme(pred_lm, gt_lm, inter_ocular_dist):\n",
    "    pred_lm = pred_lm.reshape(-1, 2)\n",
    "    gt_lm = gt_lm.reshape(-1, 2)\n",
    "    error = np.sqrt(((pred_lm - gt_lm) ** 2).sum(axis=1)).mean()\n",
    "    nme = error / inter_ocular_dist\n",
    "    return nme\n",
    "\n",
    "# CELL 4: Visualize Test Prediction\n",
    "def visualize_test_prediction(model, device, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, landmarks, poses, img_paths = next(iter(test_loader))\n",
    "        images = images.to(device)\n",
    "        img_path = img_paths[0]\n",
    "        gt_landmarks = landmarks[0].numpy().reshape(-1, 2)\n",
    "        gt_pose = poses[0].numpy()\n",
    "        \n",
    "        pred_landmarks, pred_pose = model(images)\n",
    "        pred_landmarks = pred_landmarks[0].numpy().reshape(-1, 2)\n",
    "        pred_pose = pred_pose[0].numpy()\n",
    "        \n",
    "        img = cv2.imread(os.path.join('F:/human behaviour detection/widerface/WFLW/WFLW_images', img_path))\n",
    "        if img is not None:\n",
    "            h, w = img.shape[:2]\n",
    "            gt_landmarks = gt_landmarks * [w, h]\n",
    "            pred_landmarks = pred_landmarks * [w, h]\n",
    "            \n",
    "            for (x, y) in gt_landmarks:\n",
    "                cv2.circle(img, (int(x), int(y)), 1, (0, 255, 0), -1)\n",
    "            for (x, y) in pred_landmarks:\n",
    "                cv2.circle(img, (int(x), int(y)), 1, (0, 0, 255), -1)\n",
    "            \n",
    "            cv2.putText(img, f'GT: P:{gt_pose[0]:.2f}, Y:{gt_pose[1]:.2f}, R:{gt_pose[2]:.2f}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "            cv2.putText(img, f'Pred: P:{pred_pose[0]:.2f}, Y:{pred_pose[1]:.2f}, R:{pred_pose[2]:.2f}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "            \n",
    "            cv2.imshow('Test Prediction', img)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"Visualized test image: {img_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to load: {img_path}\")\n",
    "\n",
    "# CELL 5: Test Model on WFLW Test Dataset\n",
    "def test_pose():\n",
    "    test_dataset = WFLWDataset(\n",
    "        annotation_file='F:/human behaviour detection/widerface/WFLW/WFLW_annotations/list_98pt_rect_attr_train_test/list_98pt_rect_attr_test.txt',\n",
    "        img_dir='F:/human behaviour detection/widerface/WFLW/WFLW_images'\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LightweightHRNet().to(device)\n",
    "    \n",
    "    model_path = 'landmark_pose_model.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    total_nme, total_mae_pitch, total_mae_yaw, total_mae_roll = 0.0, 0.0, 0.0, 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, landmarks, poses, _ in test_loader:\n",
    "            images, landmarks, poses = images.to(device), landmarks.to(device), poses.to(device)\n",
    "            pred_lm, pred_pose = model(images)\n",
    "            \n",
    "            # Compute NME for landmarks\n",
    "            for i in range(images.size(0)):\n",
    "                gt_lm = landmarks[i].cpu().numpy().reshape(-1, 2)\n",
    "                pred_lm_i = pred_lm[i].cpu().numpy().reshape(-1, 2)\n",
    "                # Inter-ocular distance (distance between outer eye corners, points 33 and 62 in WFLW)\n",
    "                inter_ocular_dist = np.sqrt(((gt_lm[33] - gt_lm[62]) ** 2).sum())\n",
    "                if inter_ocular_dist == 0:  # Avoid division by zero\n",
    "                    continue\n",
    "                nme = compute_nme(pred_lm_i, gt_lm, inter_ocular_dist)\n",
    "                total_nme += nme\n",
    "                \n",
    "                # Compute MAE for pose\n",
    "                gt_pose_i = poses[i].cpu().numpy()\n",
    "                pred_pose_i = pred_pose[i].cpu().numpy()\n",
    "                total_mae_pitch += abs(pred_pose_i[0] - gt_pose_i[0])\n",
    "                total_mae_yaw += abs(pred_pose_i[1] - gt_pose_i[1])\n",
    "                total_mae_roll += abs(pred_pose_i[2] - gt_pose_i[2])\n",
    "                num_samples += 1\n",
    "    \n",
    "    avg_nme = total_nme / num_samples\n",
    "    avg_mae_pitch = total_mae_pitch / num_samples\n",
    "    avg_mae_yaw = total_mae_yaw / num_samples\n",
    "    avg_mae_roll = total_mae_roll / num_samples\n",
    "    print(f\"Test Results (WFLW Metrics):\")\n",
    "    print(f\"Average NME (Landmarks): {avg_nme:.4f}\")\n",
    "    print(f\"Average MAE Pitch: {avg_mae_pitch:.2f} degrees\")\n",
    "    print(f\"Average MAE Yaw: {avg_mae_yaw:.2f} degrees\")\n",
    "    print(f\"Average MAE Roll: {avg_mae_roll:.2f} degrees\")\n",
    "    print(f\"Total Samples Evaluated: {num_samples}\")\n",
    "    \n",
    "    visualize_test_prediction(model, device, test_loader)\n",
    "\n",
    "# CELL 6: Main Execution\n",
    "if __name__ == '__main__':\n",
    "    test_pose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0ed1b8",
   "metadata": {},
   "source": [
    "real time infernece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# CELL 1: Lightweight HRNet Model\n",
    "class LightweightHRNet(nn.Module):\n",
    "    def __init__(self, num_landmarks=98):\n",
    "        super(LightweightHRNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downsample = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.stage2_high = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.stage2_low = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.upsample = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_conv = nn.Conv2d(16, 16, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc_landmarks = nn.Linear(16, num_landmarks * 2)\n",
    "        self.fc_pose = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.stage1(x)\n",
    "        x_high = self.stage2_high(x)\n",
    "        x_low = self.downsample(x)\n",
    "        x_low = self.stage2_low(x_low)\n",
    "        x_low_up = self.upsample(x_low)\n",
    "        x = x_high + x_low_up\n",
    "        x = self.final_conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        landmarks = self.fc_landmarks(x)\n",
    "        pose = self.fc_pose(x)\n",
    "        return landmarks, pose\n",
    "\n",
    "# CELL 2: Compute EAR, MAR, and Blink Rate\n",
    "def compute_ear(eye_points):\n",
    "    A = np.linalg.norm(eye_points[1] - eye_points[5])\n",
    "    B = np.linalg.norm(eye_points[2] - eye_points[4])\n",
    "    C = np.linalg.norm(eye_points[0] - eye_points[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def compute_mar(mouth_points):\n",
    "    A = np.linalg.norm(mouth_points[1] - mouth_points[7])\n",
    "    B = np.linalg.norm(mouth_points[2] - mouth_points[6])\n",
    "    C = np.linalg.norm(mouth_points[3] - mouth_points[5])\n",
    "    D = np.linalg.norm(mouth_points[0] - mouth_points[4])\n",
    "    mar = (A + B + C) / (2.0 * D)\n",
    "    return mar\n",
    "\n",
    "# CELL 3: Real-Time Sleep Behavior Classification\n",
    "def realtime_sleep_behavior_classification():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LightweightHRNet().to(device)\n",
    "    \n",
    "    model_path = 'landmark_pose_model.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    if face_cascade.empty():\n",
    "        raise RuntimeError(\"Failed to load Haar Cascade classifier\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Failed to open webcam\")\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "    \n",
    "    # Behavior detection parameters\n",
    "    EAR_THRESHOLD = 0.25  # Eye closure threshold\n",
    "    MAR_THRESHOLD = 0.5   # Yawning threshold\n",
    "    PITCH_THRESHOLD = -30  # Head tilt down threshold (degrees)\n",
    "    CONSECUTIVE_FRAMES = 90  # ~3 seconds at 30 FPS\n",
    "    BLINK_EAR_THRESHOLD = 0.25\n",
    "    BLINK_RATE_THRESHOLD = 2  # Blinks per second indicating drowsiness\n",
    "    \n",
    "    # Track states\n",
    "    eyes_closed_counter = 0\n",
    "    yawning_counter = 0\n",
    "    head_down_counter = 0\n",
    "    ear_history = deque(maxlen=30)  # Track EAR for blink detection (1 second at 30 FPS)\n",
    "    blink_counter = 0\n",
    "    last_ear = 1.0  # Initial EAR value\n",
    "    state = \"Active\"\n",
    "    \n",
    "    print(\"Press 'q' to exit the webcam feed.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            if face.size == 0:\n",
    "                continue\n",
    "            face = cv2.resize(face, (112, 112))\n",
    "            face = face.transpose(2, 0, 1) / 255.0\n",
    "            face_tensor = torch.tensor(face, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                landmarks, pose = model(face_tensor)\n",
    "                landmarks = landmarks[0].cpu().numpy().reshape(-1, 2) * [w, h] + [x, y]\n",
    "                pose = pose[0].cpu().numpy()\n",
    "                pose[0] = np.clip(pose[0], -90, 90)  # Pitch\n",
    "                pose[1] = np.clip(pose[1], -90, 90)  # Yaw\n",
    "                pose[2] = np.clip(pose[2], -180, 180)  # Roll\n",
    "            \n",
    "            # Compute features\n",
    "            left_eye = landmarks[36:42]\n",
    "            right_eye = landmarks[42:48]\n",
    "            mouth = landmarks[60:68]\n",
    "            \n",
    "            left_ear = compute_ear(left_eye)\n",
    "            right_ear = compute_ear(right_eye)\n",
    "            avg_ear = (left_ear + right_ear) / 2.0\n",
    "            mar = compute_mar(mouth)\n",
    "            pitch = pose[0]\n",
    "            \n",
    "            # Blink detection\n",
    "            ear_history.append(avg_ear)\n",
    "            if len(ear_history) >= 2:\n",
    "                if last_ear > BLINK_EAR_THRESHOLD and avg_ear <= BLINK_EAR_THRESHOLD:\n",
    "                    blink_counter += 1\n",
    "            last_ear = avg_ear\n",
    "            blink_rate = blink_counter / (len(ear_history) / 30.0)  # Blinks per second\n",
    "            \n",
    "            # State tracking\n",
    "            if avg_ear < EAR_THRESHOLD:\n",
    "                eyes_closed_counter += 1\n",
    "            else:\n",
    "                eyes_closed_counter = 0\n",
    "            \n",
    "            if mar > MAR_THRESHOLD:\n",
    "                yawning_counter += 1\n",
    "            else:\n",
    "                yawning_counter = 0\n",
    "            \n",
    "            if pitch < PITCH_THRESHOLD:\n",
    "                head_down_counter += 1\n",
    "            else:\n",
    "                head_down_counter = 0\n",
    "            \n",
    "            # Classify behavior\n",
    "            if (eyes_closed_counter >= CONSECUTIVE_FRAMES and head_down_counter >= CONSECUTIVE_FRAMES):\n",
    "                state = \"Sleeping\"\n",
    "            elif (eyes_closed_counter >= CONSECUTIVE_FRAMES or \n",
    "                  yawning_counter >= CONSECUTIVE_FRAMES or \n",
    "                  head_down_counter >= CONSECUTIVE_FRAMES or \n",
    "                  blink_rate > BLINK_RATE_THRESHOLD):\n",
    "                state = \"Drowsy\"\n",
    "            else:\n",
    "                state = \"Active\"\n",
    "            \n",
    "            # Visualize\n",
    "            for (lx, ly) in landmarks:\n",
    "                cv2.circle(frame, (int(lx), int(ly)), 1, (0, 255, 0), -1)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 1)\n",
    "            cv2.putText(frame, f'P:{pose[0]:.2f}, Y:{pose[1]:.2f}, R:{pose[2]:.2f}', \n",
    "                        (x, y-50), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "            cv2.putText(frame, f'EAR:{avg_ear:.2f}, MAR:{mar:.2f}', \n",
    "                        (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "            cv2.putText(frame, f'Blink Rate:{blink_rate:.2f}/s', \n",
    "                        (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "            color = (0, 255, 0) if state == \"Active\" else (0, 165, 255) if state == \"Drowsy\" else (0, 0, 255)\n",
    "            cv2.putText(frame, state, (x, y+h+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        cv2.imshow('Sleep Behavior Classification', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# CELL 4: Main Execution\n",
    "if __name__ == '__main__':\n",
    "    realtime_sleep_behavior_classification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_sleep_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
