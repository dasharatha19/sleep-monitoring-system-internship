{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582d4fb5",
   "metadata": {},
   "source": [
    "converting widerface annotation files to yolo format annotation file(format change) on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PATHS (update these if needed)\n",
    "root_dir = \"F:/human behaviour detection/widerface\"\n",
    "image_dir = os.path.join(root_dir, \"WIDER_train\", \"images\")\n",
    "label_dir = os.path.join(root_dir, \"WIDER_train\", \"label\")\n",
    "ann_file = os.path.join(root_dir, \"wider_face_split\", \"wider_face_train_bbx_gt.txt\")\n",
    "\n",
    "def convert_to_yolo(x, y, w, h, img_w, img_h):\n",
    "    \"\"\"Convert WIDERFACE bbox to YOLO format\"\"\"\n",
    "    x_center = (x + w/2) / img_w\n",
    "    y_center = (y + h/2) / img_h\n",
    "    return x_center, y_center, w/img_w, h/img_h\n",
    "\n",
    "def process_annotations():\n",
    "    # Create label directory if not exists\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "    \n",
    "    with open(ann_file, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    i = 0\n",
    "    stats = {'processed': 0, 'errors': 0, 'missing_images': 0}\n",
    "\n",
    "    with tqdm(total=len(lines), desc=\"Converting Annotations\") as pbar:\n",
    "        while i < len(lines):\n",
    "            # 1. Read image path (e.g. \"0--Parade/Parade_1.jpg\")\n",
    "            img_rel_path = lines[i]\n",
    "            i += 1\n",
    "            \n",
    "            # 2. Read number of faces\n",
    "            try:\n",
    "                num_faces = int(lines[i])\n",
    "            except ValueError:\n",
    "                stats['errors'] += 1\n",
    "                i += 1\n",
    "                continue\n",
    "            i += 1\n",
    "\n",
    "            # 3. Build paths\n",
    "            img_path = os.path.join(image_dir, img_rel_path.replace('/', os.sep))\n",
    "            label_path = os.path.join(label_dir, f\"{os.path.splitext(img_rel_path)[0]}.txt\")\n",
    "            os.makedirs(os.path.dirname(label_path), exist_ok=True)\n",
    "\n",
    "            # 4. Skip if image missing\n",
    "            if not os.path.exists(img_path):\n",
    "                stats['missing_images'] += 1\n",
    "                i += num_faces  # Skip bbox lines\n",
    "                continue\n",
    "\n",
    "            # 5. Get image dimensions\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img_w, img_h = img.size\n",
    "            except Exception as e:\n",
    "                stats['errors'] += 1\n",
    "                i += num_faces\n",
    "                continue\n",
    "\n",
    "            # 6. Process bounding boxes\n",
    "            with open(label_path, 'w') as lbl_file:\n",
    "                for _ in range(num_faces):\n",
    "                    if i >= len(lines):\n",
    "                        break\n",
    "                    \n",
    "                    # Parse bbox (x,y,w,h)\n",
    "                    try:\n",
    "                        x, y, w, h = map(float, lines[i].split()[:4])\n",
    "                    except:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Validate bbox\n",
    "                    if w <= 0 or h <= 0:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to YOLO format\n",
    "                    x = max(0, min(x, img_w-1))\n",
    "                    y = max(0, min(y, img_h-1))\n",
    "                    xc, yc, wn, hn = convert_to_yolo(x, y, w, h, img_w, img_h)\n",
    "                    \n",
    "                    # Write to file (class 0 for face)\n",
    "                    lbl_file.write(f\"0 {xc:.6f} {yc:.6f} {wn:.6f} {hn:.6f}\\n\")\n",
    "                    i += 1\n",
    "\n",
    "            stats['processed'] += 1\n",
    "            pbar.update(2 + num_faces)  # Update for image + num_faces lines\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n=== Conversion Summary ===\")\n",
    "    print(f\"Processed images: {stats['processed']}\")\n",
    "    print(f\"Missing images: {stats['missing_images']}\")\n",
    "    print(f\"Errors: {stats['errors']}\")\n",
    "    print(f\"Label files saved to: {label_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776d8d9",
   "metadata": {},
   "source": [
    "Annotated images(images with bounding boxes being put on main object(i.e face)) for checking whether the files has been converted properly or not, but on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def draw_boxes_on_images(image_root, label_root, output_root):\n",
    "    for root, _, files in os.walk(image_root):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".jpg\"):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(image_path, image_root)\n",
    "            image_name_wo_ext = os.path.splitext(file)[0]\n",
    "\n",
    "            label_path = os.path.join(label_root, os.path.dirname(rel_path), image_name_wo_ext + \".txt\")\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"[Warning] Missing annotation for: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                print(f\"[Error] Cannot read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            with open(label_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    print(f\"[Error] Invalid line in {label_path}: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                cls_id, x_center, y_center, width, height = map(float, parts)\n",
    "                if any(v < 0 or v > 1 for v in [x_center, y_center, width, height]):\n",
    "                    print(f\"[Warning] Out-of-range coords in {label_path}: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert YOLO format to pixel format\n",
    "                x1 = int((x_center - width / 2) * w)\n",
    "                y1 = int((y_center - height / 2) * h)\n",
    "                x2 = int((x_center + width / 2) * w)\n",
    "                y2 = int((y_center + height / 2) * h)\n",
    "\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, str(int(cls_id)), (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "            # Prepare the output path\n",
    "            output_path = os.path.join(output_root, rel_path)\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            cv2.imwrite(output_path, img)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "\n",
    "\n",
    "# === Run the function ===\n",
    "\n",
    "image_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_train\\images\"\n",
    "label_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_train\\labels\"\n",
    "output_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_train\\annotated\"\n",
    "\n",
    "draw_boxes_on_images(image_dir, label_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f563d",
   "metadata": {},
   "source": [
    "anchor boxes generation using k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def load_yolo_labels(label_dir, image_size=416):\n",
    "    box_dims = []\n",
    "    for root, _, files in os.walk(label_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    for line in f.readlines():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) != 5:\n",
    "                            continue\n",
    "                        _, x_center, y_center, width, height = map(float, parts)\n",
    "                        box_dims.append([width * image_size, height * image_size])\n",
    "    return np.array(box_dims)\n",
    "\n",
    "def iou(box, clusters):\n",
    "    x = np.minimum(clusters[:, 0], box[0])\n",
    "    y = np.minimum(clusters[:, 1], box[1])\n",
    "    intersection = x * y\n",
    "    box_area = box[0] * box[1]\n",
    "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "    union = box_area + cluster_area - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def avg_iou(boxes, clusters):\n",
    "    return np.mean([np.max(iou(box, clusters)) for box in boxes])\n",
    "\n",
    "def kmeans(boxes, k, dist=np.median, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(boxes.shape[0], k, replace=False)\n",
    "    clusters = boxes[indices]\n",
    "    last_clusters = np.zeros((boxes.shape[0],))\n",
    "    \n",
    "    while True:\n",
    "        distances = []\n",
    "        for box in boxes:\n",
    "            distances.append(1 - iou(box, clusters))\n",
    "        distances = np.array(distances)\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        if (last_clusters == nearest_clusters).all():\n",
    "            break\n",
    "\n",
    "        for i in range(k):\n",
    "            clusters[i] = dist(boxes[nearest_clusters == i], axis=0)\n",
    "        last_clusters = nearest_clusters\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# ---- SET PATH TO YOUR LABEL DIRECTORY ----\n",
    "label_dir = \"F:/human_behaviour_detection/widerface/WIDER_train/labels\"\n",
    "image_size = 416\n",
    "num_anchors = 5\n",
    "\n",
    "# Load bounding boxes\n",
    "boxes = load_yolo_labels(label_dir, image_size=image_size)\n",
    "\n",
    "# Run k-means clustering\n",
    "anchors = kmeans(boxes, k=num_anchors)\n",
    "\n",
    "# Sort anchors by area\n",
    "anchors = anchors[np.argsort(anchors[:, 0] * anchors[:, 1])]\n",
    "\n",
    "# Normalize (optional)\n",
    "anchors_normalized = anchors / image_size\n",
    "print(\"Normalized anchors (for YOLO config):\")\n",
    "print(anchors_normalized)\n",
    "\n",
    "print(\"Average IoU:\", avg_iou(boxes, anchors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927353d5",
   "metadata": {},
   "source": [
    "code for converting the labels format(widerface format ) to yolo format for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Base paths (customize these for your setup)\n",
    "val_images_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_val\\images\"\n",
    "val_labels_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_val\\labels\"\n",
    "annotation_file = r\"F:\\human behaviour detection\\widerface\\wider_face_split\\wider_face_val_bbx_gt.txt\"\n",
    "\n",
    "# Make sure output label folders exist\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "\n",
    "def convert_to_yolo(x, y, w, h, img_w, img_h):\n",
    "    x_center = (x + w / 2) / img_w\n",
    "    y_center = (y + h / 2) / img_h\n",
    "    return [0, x_center, y_center, w / img_w, h / img_h]  # class_id = 0\n",
    "\n",
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    img_rel_path = lines[i].strip()\n",
    "    i += 1\n",
    "    num_boxes = int(lines[i].strip())\n",
    "    i += 1\n",
    "\n",
    "    # Resolve full image path\n",
    "    img_path = os.path.join(val_images_dir, img_rel_path)\n",
    "    label_path = os.path.join(val_labels_dir, img_rel_path.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "    os.makedirs(os.path.dirname(label_path), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img_w, img_h = img.size\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Image not found: {img_path}\")\n",
    "        i += num_boxes\n",
    "        continue\n",
    "\n",
    "    with open(label_path, 'w') as out_f:\n",
    "        for _ in range(num_boxes):\n",
    "            parts = lines[i].strip().split()\n",
    "            i += 1\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = map(int, parts[:4])\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue  # skip invalid boxes\n",
    "\n",
    "            yolo_box = convert_to_yolo(x, y, w, h, img_w, img_h)\n",
    "            out_f.write(\" \".join(f\"{val:.6f}\" for val in yolo_box) + \"\\n\")\n",
    "\n",
    "print(\"✅ All validation annotations successfully converted to YOLO format.\")\n",
    "print(f\"Saved under: {val_labels_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947ab15",
   "metadata": {},
   "source": [
    "code to verify the whether converted format obatined are correctly detecting the object in images so craetion of folder name annoated whre the imaages has bouding box based on its correspodning label file for validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Directories\n",
    "base_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_val\"\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "labels_dir = os.path.join(base_dir, \"labels\")\n",
    "annotated_dir = os.path.join(base_dir, \"annotated\")\n",
    "\n",
    "os.makedirs(annotated_dir, exist_ok=True)\n",
    "\n",
    "def draw_boxes(img_path, label_path, save_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"❌ Could not load: {img_path}\")\n",
    "        return\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                _, x_center, y_center, width, height = parts\n",
    "\n",
    "                # Convert back to pixel coordinates\n",
    "                x1 = int((x_center - width / 2) * w)\n",
    "                y1 = int((y_center - height / 2) * h)\n",
    "                x2 = int((x_center + width / 2) * w)\n",
    "                y2 = int((y_center + height / 2) * h)\n",
    "\n",
    "                # Draw rectangle and label\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, 'face', (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ No label for: {img_path}\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "# Walk through all image files and visualize annotations\n",
    "for root, _, files in os.walk(images_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            rel_path = os.path.relpath(os.path.join(root, file), images_dir)\n",
    "            img_path = os.path.join(images_dir, rel_path)\n",
    "            label_path = os.path.join(labels_dir, rel_path.replace(\".jpg\", \".txt\"))\n",
    "            save_path = os.path.join(annotated_dir, rel_path)\n",
    "\n",
    "            draw_boxes(img_path, label_path, save_path)\n",
    "\n",
    "print(\"✅ Annotated images saved to:\", annotated_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_sleep_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
