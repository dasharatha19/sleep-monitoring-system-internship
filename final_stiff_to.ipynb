{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc35dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        # Verify directories exist\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "        if not os.path.exists(label_dir):\n",
    "            raise FileNotFoundError(f\"Label directory not found: {label_dir}\")\n",
    "\n",
    "        # Recursively find all images and labels\n",
    "        self.image_paths = glob(os.path.join(image_dir, '**/*.jpg'), recursive=True)\n",
    "        self.label_paths = glob(os.path.join(label_dir, '**/*.txt'), recursive=True)\n",
    "\n",
    "        # Create dicts for matching files by basename (without extension)\n",
    "        images_dict = {os.path.splitext(os.path.basename(p))[0]: p for p in self.image_paths}\n",
    "        labels_dict = {os.path.splitext(os.path.basename(p))[0]: p for p in self.label_paths}\n",
    "\n",
    "        # Find common keys present in both images and labels\n",
    "        common_keys = sorted(set(images_dict.keys()) & set(labels_dict.keys()))\n",
    "\n",
    "        if len(common_keys) == 0:\n",
    "            raise RuntimeError(f\"No matching image-label pairs found in {image_dir} and {label_dir}!\")\n",
    "\n",
    "        # Store pairs of matching image and label paths\n",
    "        self.pairs = [(images_dict[k], labels_dict[k]) for k in common_keys]\n",
    "\n",
    "        # Statistics: Count bounding boxes\n",
    "        total_boxes = 0\n",
    "        for _, label_path in self.pairs:\n",
    "            with open(label_path, 'r') as f:\n",
    "                boxes = [line.strip().split() for line in f.readlines() if line.strip() and len(line.strip().split()) == 5]\n",
    "                total_boxes += len(boxes)\n",
    "        print(f\"[Stats] Training dataset: {len(self.pairs)} image-label pairs, {total_boxes} total bounding boxes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.pairs[idx]\n",
    "\n",
    "        # Load image with error handling\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        # Load labels in YOLO format: class, x_center, y_center, width, height (normalized)\n",
    "        boxes = []\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        print(f\"Skipping malformed label in {label_path}: {line.strip()}\")\n",
    "                        continue\n",
    "                    cls, x, y, w, h = map(float, parts)\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                        print(f\"Warning: Invalid box coordinates in {label_path}: {[cls, x, y, w, h]}\")\n",
    "                        continue\n",
    "                    boxes.append([cls, x, y, w, h])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading label file {label_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71dd3799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stats] Training dataset: 12876 image-label pairs, 159367 total bounding boxes\n",
      "Total pairs: 12876\n",
      "[Stats] Average bounding boxes per image: 12.38\n",
      "Image type: <class 'torch.Tensor'>, size: torch.Size([3, 732, 1024])\n",
      "Labels shape: torch.Size([2, 5]), example labels: tensor([[0.0000, 0.1240, 0.5273, 0.0098, 0.0164],\n",
      "        [0.0000, 0.2461, 0.5260, 0.0117, 0.0191]])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "image_dir = \"F:\\human behaviour detection\\widerface\\WIDER_train\\images\"\n",
    "label_dir = \"F:\\human behaviour detection\\widerface\\WIDER_train\\labels\"\n",
    "\n",
    "# Basic transform for testing\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "try:\n",
    "    dataset = YOLODataset(image_dir, label_dir, transform=transform)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Total pairs: {len(dataset)}\")\n",
    "\n",
    "# Calculate average boxes per image\n",
    "total_boxes = 0\n",
    "for i in range(len(dataset)):\n",
    "    _, boxes = dataset[i]\n",
    "    if boxes is None:\n",
    "        continue\n",
    "    total_boxes += len(boxes)\n",
    "avg_boxes = total_boxes / len(dataset) if len(dataset) > 0 else 0\n",
    "print(f\"[Stats] Average bounding boxes per image: {avg_boxes:.2f}\")\n",
    "\n",
    "if len(dataset) > 0:\n",
    "    img, boxes = dataset[0]\n",
    "    if img is None or boxes is None:\n",
    "        print(\"Failed to load first sample.\")\n",
    "    else:\n",
    "        size_info = img.shape if hasattr(img, 'shape') else \"Unknown size attribute\"\n",
    "        print(f\"Image type: {type(img)}, size: {size_info}\")\n",
    "        print(f\"Labels shape: {boxes.shape}, example labels: {boxes[:5]}\")\n",
    "else:\n",
    "    print(\"Dataset is empty, no samples to show.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a114a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stats] Training dataset: 12876 image-label pairs, 159367 total bounding boxes\n",
      "Batch images shape: torch.Size([32, 3, 416, 416])\n",
      "Number of label tensors: 32\n",
      "Example label shape for first image: torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "IMG_SIZE = 416\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "try:\n",
    "    dataset = YOLODataset(image_dir, label_dir, transform=transform)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Filter out None entries (failed loads)\n",
    "    batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Batch is empty after filtering failed loads.\")\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [item[1] for item in batch]\n",
    "    return images, labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=(device.type == 'cuda'),\n",
    "        persistent_workers=False,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create DataLoader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test one batch\n",
    "try:\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Number of label tensors: {len(labels)}\")\n",
    "    print(f\"Example label shape for first image: {labels[0].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab925d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinyYOLOv2Debug(nn.Module):\n",
    "    def __init__(self, num_classes=1, B=2, S=13):\n",
    "        super(TinyYOLOv2Debug, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = num_classes\n",
    "        self.anchors = torch.tensor([[1.5, 1.5], [3.0, 3.0]])  # Match inference and loss\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.conv9 = nn.Conv2d(1024, self.B * (5 + self.C), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        print(f\"After conv9 (final conv): {x.shape}\")\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        print(f\"After permute: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97fe086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, S=13, B=2, C=1, lambda_coord=5, lambda_noobj=0.5):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        self.anchors = torch.tensor([[1.5, 1.5], [3.0, 3.0]])  # Match model and inference\n",
    "\n",
    "    def compute_iou(self, box1, box2):\n",
    "        x1, y1, w1, h1 = box1\n",
    "        x2, y2, w2, h2 = box2\n",
    "        x1_min, y1_min = x1 - w1/2, y1 - h1/2\n",
    "        x1_max, y1_max = x1 + w1/2, y1 + h1/2\n",
    "        x2_min, y2_min = x2 - w2/2, y2 - h2/2\n",
    "        x2_max, y2_max = x2 + w2/2, y2 + h2/2\n",
    "\n",
    "        inter_xmin = torch.max(x1_min, x2_min)\n",
    "        inter_ymin = torch.max(y1_min, y2_min)\n",
    "        inter_xmax = torch.min(x1_max, x2_max)\n",
    "        inter_ymax = torch.min(y1_max, y2_max)\n",
    "\n",
    "        inter_area = torch.clamp(inter_xmax - inter_xmin, min=0) * torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "        area1 = w1 * h1\n",
    "        area2 = w2 * h2\n",
    "        union_area = area1 + area2 - inter_area\n",
    "        return inter_area / union_area\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        batch_size = predictions.size(0)\n",
    "        device = predictions.device\n",
    "        self.anchors = self.anchors.to(device)\n",
    "\n",
    "        predictions = predictions.reshape(batch_size, self.S, self.S, self.B, 5 + self.C)\n",
    "\n",
    "        coord_loss = 0\n",
    "        conf_loss = 0\n",
    "        class_loss = 0\n",
    "        num_objects = 0\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            target_tensor = torch.zeros(self.S, self.S, self.B, 5 + self.C, device=device)\n",
    "\n",
    "            for box in targets[b]:\n",
    "                cls, x, y, w, h = box\n",
    "                i = int(x * self.S)\n",
    "                j = int(y * self.S)\n",
    "                if i >= self.S or j >= self.S or i < 0 or j < 0:\n",
    "                    continue\n",
    "                gt_box = torch.tensor([x, y, w, h], device=device)\n",
    "\n",
    "                best_iou = 0\n",
    "                best_k = 0\n",
    "                for k in range(self.B):\n",
    "                    anchor = self.anchors[k]\n",
    "                    anchor_box = torch.tensor([x, y, anchor[0], anchor[1]], device=device)\n",
    "                    iou = self.compute_iou(gt_box, anchor_box)\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_k = k\n",
    "\n",
    "                target_tensor[j, i, best_k, 0] = 1\n",
    "                target_tensor[j, i, best_k, 1:5] = torch.tensor([x * self.S - i, y * self.S - j, w, h], device=device)\n",
    "                target_tensor[j, i, best_k, 5 + int(cls)] = 1\n",
    "                num_objects += 1\n",
    "\n",
    "            obj_mask = target_tensor[..., 0]\n",
    "            noobj_mask = 1 - obj_mask\n",
    "\n",
    "            pred_x = torch.sigmoid(predictions[b][..., 1])\n",
    "            pred_y = torch.sigmoid(predictions[b][..., 2])\n",
    "            pred_w = self.anchors[:, 0] * torch.exp(predictions[b][..., 3])\n",
    "            pred_h = self.anchors[:, 1] * torch.exp(predictions[b][..., 4])\n",
    "            pred_coords = torch.stack([pred_x, pred_y, pred_w, pred_h], dim=-1)\n",
    "\n",
    "            if obj_mask.sum() > 0:\n",
    "                coord_loss += self.lambda_coord * self.mse(\n",
    "                    pred_coords[obj_mask == 1],\n",
    "                    target_tensor[obj_mask == 1][:, 1:5]\n",
    "                )\n",
    "                class_loss += self.mse(\n",
    "                    predictions[b][obj_mask == 1][:, 5:],\n",
    "                    target_tensor[obj_mask == 1][:, 5:]\n",
    "                )\n",
    "\n",
    "            conf_loss += self.mse(\n",
    "                torch.sigmoid(predictions[b][..., 0])[obj_mask == 1],\n",
    "                target_tensor[..., 0][obj_mask == 1]\n",
    "            )\n",
    "            conf_loss += self.lambda_noobj * self.mse(\n",
    "                torch.sigmoid(predictions[b][..., 0])[noobj_mask == 1],\n",
    "                target_tensor[..., 0][noobj_mask == 1]\n",
    "            )\n",
    "\n",
    "        if num_objects == 0:\n",
    "            print(\"[Debug] No objects in batch, returning conf_loss only\")\n",
    "            return conf_loss\n",
    "\n",
    "        total_loss = (coord_loss + conf_loss + class_loss) / max(num_objects, 1)\n",
    "        print(f\"[Debug] Loss components - Coord: {coord_loss:.4f}, Conf: {conf_loss:.4f}, Class: {class_loss:.4f}, Total: {total_loss:.4f}\")\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f34fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "Model output shape: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 167.2659, Conf: 13.7766, Class: 34.9846, Total: 1.2415\n",
      "Initial loss (untrained): 1.2415\n",
      "Completed one training step successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    model = TinyYOLOv2Debug(num_classes=1, B=2, S=13).to(device)\n",
    "    criterion = YOLOLoss(S=13, B=2, C=1).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing model, criterion, or optimizer: {e}\")\n",
    "    raise\n",
    "\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    images, labels = next(iter(dataloader))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch from dataloader: {e}\")\n",
    "    raise\n",
    "\n",
    "images = images.to(device)\n",
    "labels = [label.to(device) for label in labels]\n",
    "\n",
    "assert images.size(0) == len(labels), \"Batch size mismatch between images and labels\"\n",
    "\n",
    "try:\n",
    "    outputs = model(images)\n",
    "    print(f\"Model output shape: {outputs.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model forward pass: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(f\"Initial loss (untrained): {loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error computing loss: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "except Exception as e:\n",
    "    print(f\"Error during optimization step: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Completed one training step successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0591d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using validation images from: F:\\human behaviour detection\\widerface\\WIDER_val\\images\n",
      "Using validation labels from: F:\\human behaviour detection\\widerface\\WIDER_val\\labels\n",
      "[Stats] Validation dataset: 3226 image-label pairs, 39697 total bounding boxes\n",
      "Validation samples: 3226\n",
      "Validation batch images shape: torch.Size([32, 3, 416, 416])\n",
      "Validation batch labels count: 32\n",
      "Example label shape for first image: torch.Size([39, 5])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class YOLOValidDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise FileNotFoundError(f\"Validation image directory not found: {image_dir}\")\n",
    "        if not os.path.exists(label_dir):\n",
    "            raise FileNotFoundError(f\"Validation label directory not found: {label_dir}\")\n",
    "\n",
    "        self.image_paths = glob(os.path.join(image_dir, '**/*.jpg'), recursive=True)\n",
    "        self.label_paths = glob(os.path.join(label_dir, '**/*.txt'), recursive=True)\n",
    "\n",
    "        images_dict = {os.path.splitext(os.path.basename(p))[0]: p for p in self.image_paths}\n",
    "        labels_dict = {os.path.splitext(os.path.basename(p))[0]: p for p in self.label_paths}\n",
    "\n",
    "        common_keys = sorted(set(images_dict.keys()) & set(labels_dict.keys()))\n",
    "\n",
    "        if len(common_keys) == 0:\n",
    "            raise RuntimeError(f\"No matching validation image-label pairs found in {image_dir} and {label_dir}!\")\n",
    "\n",
    "        self.pairs = [(images_dict[k], labels_dict[k]) for k in common_keys]\n",
    "\n",
    "        total_boxes = 0\n",
    "        for _, label_path in self.pairs:\n",
    "            with open(label_path, 'r') as f:\n",
    "                boxes = [line.strip().split() for line in f.readlines() if line.strip() and len(line.strip().split()) == 5]\n",
    "                total_boxes += len(boxes)\n",
    "        print(f\"[Stats] Validation dataset: {len(self.pairs)} image-label pairs, {total_boxes} total bounding boxes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.pairs[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading validation image {img_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        boxes = []\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        print(f\"Skipping malformed label in {label_path}: {line.strip()}\")\n",
    "                        continue\n",
    "                    cls, x, y, w, h = map(float, parts)\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                        print(f\"Warning: Invalid box coordinates in {label_path}: {[cls, x, y, w, h]}\")\n",
    "                        continue\n",
    "                    boxes.append([cls, x, y, w, h])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading validation label file {label_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 5), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, boxes\n",
    "\n",
    "val_image_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_val\\images\"\n",
    "val_label_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_val\\labels\"\n",
    "\n",
    "print(f\"Using validation images from: {val_image_dir}\")\n",
    "print(f\"Using validation labels from: {val_label_dir}\")\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "try:\n",
    "    valid_dataset = YOLOValidDataset(val_image_dir, val_label_dir, transform=val_transform)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create validation dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Validation batch is empty after filtering failed loads.\")\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [item[1] for item in batch]\n",
    "    return images, labels\n",
    "\n",
    "try:\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=valid_collate_fn\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create validation DataLoader: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "if len(valid_dataset) > 0:\n",
    "    try:\n",
    "        images, labels = next(iter(valid_dataloader))\n",
    "        print(f\"Validation batch images shape: {images.shape}\")\n",
    "        print(f\"Validation batch labels count: {len(labels)}\")\n",
    "        if len(labels) > 0:\n",
    "            print(f\"Example label shape for first image: {labels[0].shape}\")\n",
    "        else:\n",
    "            print(\"No labels in the first validation batch.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading validation batch: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Validation dataset is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[Stats] Training batches: 403\n",
      "[Stats] Validation batches: 101\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 176.4161, Conf: 11.4210, Class: 60.5766, Total: 0.6353\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 3706.6863, Conf: 10.1005, Class: 27.8180, Total: 7.7208\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 1157.9519, Conf: 10.5094, Class: 30.8645, Total: 4.1788\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 42166.5430, Conf: 10.1773, Class: 31.3156, Total: 150.2065\n",
      "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
      "After permute: torch.Size([32, 13, 13, 12])\n",
      "[Debug] Loss components - Coord: 1704.4755, Conf: 9.6820, Class: 40.9305, Total: 6.3361\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS = 12  # Reduced to 12 epochs for CPU\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    model = TinyYOLOv2Debug(num_classes=1, B=2, S=13).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    criterion = YOLOLoss(S=13, B=2, C=1).to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing training components: {e}\")\n",
    "    raise\n",
    "\n",
    "# Print dataset stats\n",
    "print(f\"[Stats] Training batches: {len(dataloader)}\")\n",
    "print(f\"[Stats] Validation batches: {len(valid_dataloader)}\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            targets = [t.to(device) for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training epoch {epoch+1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_dataloader:\n",
    "                images = images.to(device)\n",
    "                labels = [label.to(device) for label in labels]\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during validation epoch {epoch+1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        improvement = best_val_loss - val_loss if best_val_loss != float('inf') else 0\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        try:\n",
    "            torch.save(model.state_dict(), \"C:/widerface/best_model.pth\")\n",
    "            print(f\"Best model saved with validation loss: {best_val_loss:.4f} (Improvement: {improvement:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience counter: {counter}/{patience}\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] completed in {(time.time() - start_time):.2f}s\")\n",
    "\n",
    "print(f\"Training completed in {(time.time() - total_start_time):.2f}s. Best model saved at C:/widerface/best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e3a61",
   "metadata": {},
   "source": [
    "cell 2 output \n",
    "[Stats] Training dataset: 12876 image-label pairs, 159367 total bounding boxes\n",
    "Total pairs: 12876\n",
    "[Stats] Average bounding boxes per image: 12.38\n",
    "Image type: <class 'torch.Tensor'>, size: torch.Size([3, 732, 1024])\n",
    "Labels shape: torch.Size([2, 5]), example labels: tensor([[0.0000, 0.1240, 0.5273, 0.0098, 0.0164],\n",
    "        [0.0000, 0.2461, 0.5260, 0.0117, 0.0191]])\n",
    "cell 3 output \n",
    "[Stats] Training dataset: 12876 image-label pairs, 159367 total bounding boxes\n",
    "Batch images shape: torch.Size([32, 3, 416, 416])\n",
    "Number of label tensors: 32\n",
    "Example label shape for first image: torch.Size([6, 5])\n",
    "cell 6 output\n",
    "Using device: cpu\n",
    "After conv9 (final conv): torch.Size([32, 12, 13, 13])\n",
    "After permute: torch.Size([32, 13, 13, 12])\n",
    "Model output shape: torch.Size([32, 13, 13, 12])\n",
    "[Debug] Loss components - Coord: 167.2659, Conf: 13.7766, Class: 34.9846, Total: 1.2415\n",
    "Initial loss (untrained): 1.2415\n",
    "Completed one training step successfully.                                                                                                                   \n",
    "cell 7 output \n",
    "Using validation images from: F:\\human behaviour detection\\widerface\\WIDER_val\\images\n",
    "Using validation labels from: F:\\human behaviour detection\\widerface\\WIDER_val\\labels\n",
    "[Stats] Validation dataset: 3226 image-label pairs, 39697 total bounding boxes\n",
    "Validation samples: 3226\n",
    "Validation batch images shape: torch.Size([32, 3, 416, 416])\n",
    "Validation batch labels count: 32\n",
    "Example label shape for first image: torch.Size([39, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "criterion = YOLOLoss(S=13, B=2, C=1).to(device)\n",
    "\n",
    "val_loss = 0.0\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = [label.to(device) for label in labels]\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "except Exception as e:\n",
    "    print(f\"Error during validation: {e}\")\n",
    "    raise\n",
    "\n",
    "val_loss /= len(valid_dataloader)\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caaa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "model_path = r\"C:\\widerface\\best_model.pth\"\n",
    "test_image_dir = r\"F:\\human behaviour detection\\widerface\\WIDER_test\\images\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    model = TinyYOLOv2Debug(num_classes=1, B=2, S=13)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def decode_yolo_predictions(preds, conf_thresh=0.05, iou_thresh=0.5, img_size=416):\n",
    "    batch_size, S, _, num_attribs = preds.shape\n",
    "    B, C = 2, 1\n",
    "    cell_size = img_size / S\n",
    "\n",
    "    anchors = torch.tensor([[1.5, 1.5], [3.0, 3.0]], device=preds.device)\n",
    "\n",
    "    preds = preds.view(batch_size, S, S, B, 5 + C)\n",
    "    confidences = torch.sigmoid(preds[..., 0])\n",
    "    box_coords = preds[..., 1:5]\n",
    "    class_probs = torch.sigmoid(preds[..., 5:])\n",
    "    conf_scores = confidences * class_probs[..., 0]\n",
    "\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_classes = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        classes = []\n",
    "\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                for k in range(B):\n",
    "                    if conf_scores[b, i, j, k] > conf_thresh:\n",
    "                        x_center = (torch.sigmoid(box_coords[b, i, j, k, 0]) + j) * cell_size\n",
    "                        y_center = (torch.sigmoid(box_coords[b, i, j, k, 1]) + i) * cell_size\n",
    "                        anchor_w = anchors[k, 0]\n",
    "                        anchor_h = anchors[k, 1]\n",
    "                        w = anchor_w * torch.exp(box_coords[b, i, j, k, 2]) * cell_size\n",
    "                        h = anchor_h * torch.exp(box_coords[b, i, j, k, 3]) * cell_size\n",
    "\n",
    "                        w = torch.abs(w)\n",
    "                        h = torch.abs(h)\n",
    "                        if w <= 0 or h <= 0:\n",
    "                            continue\n",
    "\n",
    "                        x1 = x_center - w / 2\n",
    "                        y1 = y_center - h / 2\n",
    "                        x2 = x_center + w / 2\n",
    "                        y2 = y_center + h / 2\n",
    "\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        scores.append(conf_scores[b, i, j, k])\n",
    "                        classes.append(0)\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, device=preds.device)\n",
    "            scores = torch.tensor(scores, device=preds.device)\n",
    "            classes = torch.tensor(classes, device=preds.device, dtype=torch.long)\n",
    "\n",
    "            if boxes.shape[0] > 0:\n",
    "                boxes_normalized = boxes / img_size\n",
    "                keep = torchvision.ops.nms(boxes_normalized, scores, iou_thresh)\n",
    "                boxes = boxes[keep]\n",
    "                scores = scores[keep]\n",
    "                classes = classes[keep]\n",
    "            else:\n",
    "                boxes = torch.empty((0, 4), device=preds.device)\n",
    "                scores = torch.empty((0,), device=preds.device)\n",
    "                classes = torch.empty((0,), device=preds.device, dtype=torch.long)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), device=preds.device)\n",
    "            scores = torch.empty((0,), device=preds.device)\n",
    "            classes = torch.empty((0,), device=preds.device, dtype=torch.long)\n",
    "\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(scores)\n",
    "        all_classes.append(classes)\n",
    "\n",
    "    return all_boxes[0], all_scores[0], all_classes[0]\n",
    "\n",
    "def visualize_prediction(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test image {image_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    orig_w, orig_h = image.size\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            preds = model(img_tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference on {image_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    boxes, scores, classes = decode_yolo_predictions(preds, conf_thresh=0.05)\n",
    "\n",
    "    print(f\"[Stats] {os.path.basename(image_path)}: {len(boxes)} detections\")\n",
    "\n",
    "    if boxes.numel() == 0:\n",
    "        print(f\"No detections in: {os.path.basename(image_path)}\")\n",
    "        return\n",
    "\n",
    "    scale_x = orig_w / 416\n",
    "    scale_y = orig_h / 416\n",
    "    boxes[:, 0] *= scale_x\n",
    "    boxes[:, 2] *= scale_x\n",
    "    boxes[:, 1] *= scale_y\n",
    "    boxes[:, 3] *= scale_y\n",
    "\n",
    "    img_draw = F.to_tensor(image).mul(255).to(torch.uint8)\n",
    "    img_with_boxes = draw_bounding_boxes(img_draw, boxes=boxes, colors=\"red\", width=2)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img_with_boxes.permute(1, 2, 0).cpu())\n",
    "    plt.axis('off')\n",
    "    plt.title(os.path.basename(image_path))\n",
    "    plt.show()\n",
    "\n",
    "if not os.path.exists(test_image_dir):\n",
    "    raise FileNotFoundError(f\"Test image directory not found: {test_image_dir}\")\n",
    "\n",
    "image_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "test_image_paths = [\n",
    "    os.path.join(test_image_dir, f)\n",
    "    for f in os.listdir(test_image_dir)\n",
    "    if os.path.splitext(f)[1].lower() in image_extensions\n",
    "]\n",
    "\n",
    "total_detections = 0\n",
    "for img_path in test_image_paths:\n",
    "    visualize_prediction(img_path)\n",
    "    boxes, _, _ = decode_yolo_predictions(model(transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)), conf_thresh=0.05)\n",
    "    total_detections += len(boxes)\n",
    "\n",
    "print(f\"[Stats] Total detections across {len(test_image_paths)} test images: {total_detections}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((416, 416)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "class TinyYOLOv2Debug(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, B=2, S=13):\n",
    "        super(TinyYOLOv2Debug, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = num_classes\n",
    "        self.anchors = torch.tensor([[1.5, 1.5], [3.0, 3.0]])\n",
    "\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 16, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 256, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(256, 512, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.conv7 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.conv8 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1024, 1024, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(1024),\n",
    "            torch.nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.conv9 = torch.nn.Conv2d(1024, self.B * (5 + self.C), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n",
    "\n",
    "def decode_yolo_predictions(preds, conf_thresh=0.05, iou_thresh=0.5, img_size=416):\n",
    "    batch_size, S, _, num_attribs = preds.shape\n",
    "    B, C = 2, 1\n",
    "    cell_size = img_size / S\n",
    "\n",
    "    anchors = torch.tensor([[1.5, 1.5], [3.0, 3.0]], device=preds.device)\n",
    "\n",
    "    preds = preds.view(batch_size, S, S, B, 5 + C)\n",
    "    confidences = torch.sigmoid(preds[..., 0])\n",
    "    box_coords = preds[..., 1:5]\n",
    "    class_probs = torch.sigmoid(preds[..., 5:])\n",
    "    conf_scores = confidences * class_probs[..., 0]\n",
    "\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_classes = []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        classes = []\n",
    "\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                for k in range(B):\n",
    "                    if conf_scores[b, i, j, k] > conf_thresh:\n",
    "                        x_center = (torch.sigmoid(box_coords[b, i, j, k, 0]) + j) * cell_size\n",
    "                        y_center = (torch.sigmoid(box_coords[b, i, j, k, 1]) + i) * cell_size\n",
    "                        anchor_w = anchors[k, 0]\n",
    "                        anchor_h = anchors[k, 1]\n",
    "                        w = anchor_w * torch.exp(box_coords[b, i, j, k, 2]) * cell_size\n",
    "                        h = anchor_h * torch.exp(box_coords[b, i, j, k, 3]) * cell_size\n",
    "\n",
    "                        w = torch.abs(w)\n",
    "                        h = torch.abs(h)\n",
    "                        if w <= 0 or h <= 0:\n",
    "                            continue\n",
    "\n",
    "                        x1 = x_center - w / 2\n",
    "                        y1 = y_center - h / 2\n",
    "                        x2 = x_center + w / 2\n",
    "                        y2 = y_center + h / 2\n",
    "\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        scores.append(conf_scores[b, i, j, k])\n",
    "                        classes.append(0)\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, device=preds.device)\n",
    "            scores = torch.tensor(scores, device=preds.device)\n",
    "            classes = torch.tensor(classes, device=preds.device, dtype=torch.long)\n",
    "\n",
    "            if boxes.shape[0] > 0:\n",
    "                boxes_normalized = boxes / img_size\n",
    "                keep = torchvision.ops.nms(boxes_normalized, scores, iou_thresh)\n",
    "                boxes = boxes[keep]\n",
    "                scores = scores[keep]\n",
    "                classes = classes[keep]\n",
    "            else:\n",
    "                boxes = torch.empty((0, 4), device=preds.device)\n",
    "                scores = torch.empty((0,), device=preds.device)\n",
    "                classes = torch.empty((0,), device=preds.device, dtype=torch.long)\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), device=preds.device)\n",
    "            scores = torch.empty((0,), device=preds.device)\n",
    "            classes = torch.empty((0,), device=preds.device, dtype=torch.long)\n",
    "\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(scores)\n",
    "        all_classes.append(classes)\n",
    "\n",
    "    return all_boxes[0], all_scores[0], all_classes[0]\n",
    "\n",
    "def realtime_inference(model, device, conf_thresh=0.05, iou_thresh=0.5):\n",
    "    model.eval()\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(\"Error: Could not open webcam\")\n",
    "    except Exception as e:\n",
    "        print(f\"Webcam error: {e}\")\n",
    "        return\n",
    "\n",
    "    total_frames = 0\n",
    "    total_detections = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            total_frames += 1\n",
    "            frame_h, frame_w = frame.shape[:2]\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = transform(rgb_frame).unsqueeze(0).to(device)\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    preds = model(input_tensor)\n",
    "                    boxes, scores, classes = decode_yolo_predictions(preds, conf_thresh, iou_thresh)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference: {e}\")\n",
    "                continue\n",
    "\n",
    "            total_detections += len(boxes)\n",
    "            print(f\"[Stats] Frame {total_frames}: {len(boxes)} detections\")\n",
    "\n",
    "            img_tensor = torch.from_numpy(rgb_frame).permute(2, 0, 1).to(device)\n",
    "            img_tensor = img_tensor.to(torch.uint8)\n",
    "\n",
    "            if boxes.numel() > 0:\n",
    "                valid_mask = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "                boxes = boxes[valid_mask]\n",
    "                scores = scores[valid_mask]\n",
    "                classes = classes[valid_mask]\n",
    "\n",
    "                scale_x = frame_w / 416\n",
    "                scale_y = frame_h / 416\n",
    "                boxes[:, 0] *= scale_x\n",
    "                boxes[:, 2] *= scale_x\n",
    "                boxes[:, 1] *= scale_y\n",
    "                boxes[:, 3] *= scale_y\n",
    "\n",
    "                if boxes.numel() > 0:\n",
    "                    boxes = boxes.to(torch.int)\n",
    "                    img_with_boxes = draw_bounding_boxes(img_tensor, boxes=boxes, colors=\"red\", width=2)\n",
    "                    display_frame = img_with_boxes.permute(1, 2, 0).cpu().numpy()\n",
    "                    display_frame = cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR)\n",
    "                else:\n",
    "                    display_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "                    cv2.putText(display_frame, \"No faces detected\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                display_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)\n",
    "                cv2.putText(display_frame, \"No faces detected\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "            cv2.putText(display_frame, f\"FPS: {fps:.1f}\", (10, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('YOLOv2 Face Detection', display_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"[Stats] Total frames: {total_frames}, Total detections: {total_detections}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = r\"C:\\widerface\\best_model.pth\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = TinyYOLOv2Debug(num_classes=1, B=2, S=13)\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit()\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"Running on device: {device}\")\n",
    "\n",
    "    realtime_inference(model, device, conf_thresh=0.05, iou_thresh=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_sleep_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
